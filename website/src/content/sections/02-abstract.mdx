---
title: "Abstract"
order: 2
description: "Research summary and key contributions"
---

import Figure from '../../components/Figure.astro';

## Abstract

{/* TODO: Replace with your actual abstract from your research paper */}
**Vision-language-action (VLA) models** have shown promise in enabling robots to follow natural language instructions for manipulation tasks. However, existing approaches typically require large-scale datasets and expensive robotic platforms. We present a novel approach that combines pre-trained VLA models with on-robot reinforcement learning to achieve effective manipulation on a low-cost 3-degree-of-freedom (3-DoF) robotic arm. Our method leverages sparse demonstrations and PPO-based fine-tuning to adapt foundation models to resource-constrained embodiments. We evaluate our approach across 12 manipulation tasks and demonstrate significant improvements in sample efficiency and task success rates compared to baseline methods.

{/* TODO: Update this abstract with your actual research problem, approach, key results, and impact */}