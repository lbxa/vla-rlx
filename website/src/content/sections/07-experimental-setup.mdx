---
title: "Experimental Setup"
order: 7
description: "Robot specifications, hardware, and evaluation protocols"
---

import Figure from '../../components/Figure.astro';
import Table from '../../components/Table.astro';

## Experimental Setup

This section details the complete experimental infrastructure, from hardware specifications to evaluation protocols, ensuring reproducibility of our results.

### Robot Hardware

{/* TODO: Update with your actual robot specifications */}

**Platform:** Low-cost 3-DoF robotic arm (custom-built)

- **Degrees of Freedom:** 3 revolute joints (shoulder, elbow, wrist)
- **Workspace:** 30cm radius hemisphere
- **Actuators:** Dynamixel XL430-W250-T servo motors
- **Gripper:** Parallel jaw gripper (0-65mm opening)
- **Total Cost:** ~$200 USD for complete arm assembly
- **Weight:** 850g (arm + gripper)
- **Payload Capacity:** 200g max

<Figure 
  src="/images/grad-norm.svg" 
  caption="Figure 4: Hardware setup showing the 3-DoF robotic arm with parallel jaw gripper, RGB camera, and workspace layout. The arm operates on a 60×80cm tabletop with various manipulation objects."
  alt="Robot hardware photo"
/>

{/* TODO: Add actual photo of your robot hardware */}

### Sensors & Perception

**Camera Setup:**
- **Model:** Intel RealSense D435i
- **Resolution:** 640×480 RGB at 30fps (downsampled to 224×224 for model input)
- **Mounting:** Fixed third-person view, 45° angle, 60cm from workspace
- **Field of View:** Covers entire 30×40cm manipulation area
- **Calibration:** Hand-eye calibration using ArUco markers

**Proprioception:**
- Joint angles: 12-bit resolution encoders (±0.088° accuracy)
- Joint velocities: Finite-difference approximation at 100Hz
- Gripper state: Binary open/closed sensor

### Compute & Control

**Hardware:**
- **GPU:** NVIDIA RTX 3060 (12GB VRAM)
- **CPU:** Intel i7-12700K (12 cores)
- **RAM:** 32GB DDR4
- **OS:** Ubuntu 22.04 LTS

**Software Stack:**
- **Framework:** PyTorch 2.1, ROS2 Humble
- **Control Loop:** 10Hz policy execution, 100Hz low-level motor control
- **Latency Breakdown:**
  - Image capture & preprocessing: 22ms
  - Model inference (VLA forward pass): 48ms  
  - Post-processing & action smoothing: 14ms
  - Communication to motors: 10ms
  - **Total:** 94ms average end-to-end

### Training Dataset

{/* TODO: Fill in your actual dataset details */}

**Pre-training Data:**
- **Source:** OpenVLA model pre-trained on Open-X-Embodiment dataset
- **Scale:** 1M+ trajectories across 22 robot embodiments
- **Tasks:** 850+ distinct manipulation tasks

**Fine-tuning Data (On-Robot):**
- **Episodes per task:** 50 successful demonstrations
- **Episode length:** 20-60 seconds (200-600 timesteps at 10Hz)
- **Data collection:** Mix of teleoperation (30 episodes) + online RL (20 episodes)
- **Total fine-tuning data:** 600 episodes across 12 tasks = ~5 hours of robot interaction
- **Wall-clock time:** 2.5 hours per task (including resets)

### Task Suite

We evaluate on **12 manipulation tasks** covering key robotic skills:

| Task ID | Task Name | Success Criterion | Avg. Episode Length |
|---------|-----------|-------------------|---------------------|
| T1 | Pick and place cube | Cube in target zone (±2cm) | 8.2s |
| T2 | Stack two blocks | Top block stable for 3s | 12.5s |
| T3 | Push object to goal | Object in target (±3cm) | 6.8s |
| T4 | Open drawer | Drawer open &gt;8cm | 10.3s |
| T5 | Close drawer | Drawer closed (1cm gap) | 9.1s |
| T6 | Press button | Button pressed (visual detect) | 5.4s |
| T7 | Pick specific color | Correct object grasped | 9.6s |
| T8 | Sort objects | Objects in correct bins | 18.7s |
| T9 | Reorient object | Object upright (±15°) | 11.2s |
| T10 | Slide object | Object moved &gt;10cm | 7.8s |
| T11 | Grasp from clutter | Target object extracted | 13.4s |
| T12 | Follow trajectory | End-effector within 2cm of path | 15.9s |

{/* TODO: Update table with your actual tasks and metrics */}

### Evaluation Protocol

**Test Procedure:**
- **Trials per task:** 30 independent rollouts
- **Reset policy:** Manual reset to consistent initial state
- **Success criterion:** Task-specific (see table above)
- **Timeout:** 60 seconds per episode
- **Intervention policy:** Human safety stop if collision detected
- **Randomization:** 
  - Object positions: ±3cm random offset
  - Object orientations: ±30° random rotation
  - Lighting: 3 lighting conditions (bright, dim, side-lit)
  - Instruction phrasing: 5 paraphrases per task

**Metrics Reported:**
- **Success rate** (primary): Percentage of successful trials
- **Episode length**: Time to task completion
- **Intervention rate**: Human stops per 100 episodes
- **Sample efficiency**: Success rate vs. training episodes

**Statistical Analysis:**
- Confidence intervals: 95% Wilson score intervals
- Significance tests: Two-tailed t-tests with Bonferroni correction
- Sample size: N=30 per condition (Cohen's d ≥ 0.5 detectable)

### Safety Procedures

- **Workspace bounds:** Virtual walls enforced via software limits
- **Emergency stop:** Physical e-stop button within arm's reach
- **Collision detection:** Force threshold triggers automatic halt
- **Human supervision:** All experiments conducted with operator present
- **Speed limits:** Joint velocities capped at 50% of motor maximum

### Computational Budget

{/* TODO: Update with your actual training costs */}

**Pre-training:** N/A (using existing OpenVLA weights)

**Fine-tuning per task:**
- **GPU hours:** 4.5 hours (RTX 3060)
- **Real-world robot time:** 2.5 hours
- **Total wall-clock:** 3.0 hours (parallel RL training + data collection)
- **Energy cost:** ~0.5 kWh per task
- **Estimated cost:** $2-3 per task (at $0.50/GPU-hour)

**Total Experimental Budget:**
- **12 tasks × 3.0 hours:** 36 hours total wall-clock time
- **GPU cost:** $24-36 for all experiments
- **Robot wear:** ~30 hours of operation