---
title: "Model & Data Cards"
order: 8
description: "Model card and data card for transparency and reproducibility"
---

# Model & Data Cards

Following best practices from Mitchell et al. (2019) and Gebru et al. (2018), we provide detailed model and data cards to promote transparency, reproducibility, and responsible use.

{/* References:
    - Mitchell et al. "Model Cards for Model Reporting" (FAT* 2019)
    - Gebru et al. "Datasheets for Datasets" (arXiv:1803.09010)
*/}

## Model Card

### Model Overview

{/* TODO: Update with your actual model details */}

**Model Name:** VLA-3DoF-v1  
**Version:** 1.0.0  
**Release Date:** 2024-11  
**Model Type:** Vision-Language-Action Policy  
**Architecture:** Transformer-based VLA with LoRA adaptation  
**License:** MIT License

**Quick Description:**  
A vision-language-action model adapted from OpenVLA for low-cost 3-DoF robotic manipulation. The model takes RGB images and natural language instructions as input and outputs joint velocities and gripper commands.

### Intended Use

**Primary Intended Uses:**
- Research in vision-language-action models
- Educational demonstrations of VLA systems
- Prototyping manipulation tasks on low-cost robots
- Data collection for robotics research
- Benchmarking embodiment adaptation methods

**Primary Intended Users:**
- Robotics researchers
- Machine learning practitioners
- Educators in AI/robotics courses
- Students learning about VLA systems

**Out-of-Scope Uses:**
- Production deployment without human supervision
- Safety-critical applications (medical, automotive, industrial)
- High-precision tasks requiring &lt;1mm accuracy
- Heavy-duty manipulation (&gt;200g payload)
- Outdoor or uncontrolled environments
- Real-time applications requiring &lt;50ms latency

### Model Architecture

**Input Specifications:**
- **Vision:** 224×224 RGB images (normalized)
- **Language:** Variable-length text instructions (max 128 tokens)
- **Proprioception:** 3 joint angles, 3 joint velocities, 1 gripper state

**Output Specifications:**
- **Actions:** 3 joint velocities (-1.0 to 1.0, normalized)
- **Gripper:** Binary open/close command
- **Frequency:** 10Hz control rate

**Architecture Details:**
- **Visual Encoder:** CLIP ViT-B/16 (frozen)
- **Language Encoder:** Sentence-BERT (frozen)
- **Policy Backbone:** 8-layer Transformer (512 hidden dim)
- **Adaptation:** LoRA rank-16 fine-tuning
- **Parameters:** 850M total, 22M trainable
- **Precision:** FP16 inference

### Training Data

**Pre-training:**
- **Dataset:** Open-X-Embodiment via OpenVLA
- **Scale:** 1M+ trajectories, 22 embodiments
- **Tasks:** 850+ manipulation tasks
- **Note:** Pre-trained weights used as-is, no modification

**Fine-tuning:**
- **Source:** On-robot data collected on custom 3-DoF arm
- **Collection:** 50 episodes per task × 12 tasks = 600 episodes
- **Duration:** ~5 hours total robot interaction
- **Data Mix:** 60% teleoperation, 40% online RL
- **Environment:** Indoor lab, tabletop workspace
- **Objects:** 30+ household items (blocks, cups, toys, tools)

See Data Card section below for complete dataset details.

### Evaluation Data

**Test Distribution:**
- **Tasks:** Same 12 tasks as training
- **Objects:** Same object categories, novel instances
- **Conditions:** 3 lighting settings, random perturbations
- **Trials:** 30 per task = 360 total test rollouts

**Out-of-Distribution Testing:**
- Novel object categories (5 categories, 10 objects)
- Extreme lighting conditions
- Instruction paraphrasing (5 variations per task)
- Object position randomization (±3cm)

### Performance Metrics

{/* TODO: Update with your actual evaluation metrics */}

**In-Distribution:**
- **Success Rate:** 85.3% ± 3.2% (95% CI)
- **Episode Length:** 10.4s average
- **Intervention Rate:** 0.51 per hour

**Out-of-Distribution:**
- **Novel Objects:** 42.3% ± 8.7%
- **Novel Instructions:** 78.6% ± 4.1%
- **Lighting Variation:** 53.2% - 85.3%

**Latency:**
- **Inference Time:** 48ms average (94ms end-to-end)
- **Throughput:** 20.8 FPS on RTX 3060

### Limitations & Biases

**Known Limitations:**
- Restricted to 3-DoF workspace (30cm radius)
- Requires controlled lighting (50-1000 lux)
- Limited to objects &lt;200g weight
- Performance degrades on novel object categories
- English-only language understanding

**Potential Biases:**
- Training data primarily features Western household objects
- Right-handed manipulation conventions
- Bias toward common object shapes (cubes, cylinders)
- May underperform on non-standard color schemes

**Failure Modes:**
- Perception errors under poor lighting (28.3% of failures)
- Grasp failures on smooth/irregular objects (24.5%)
- Planning inefficiencies leading to timeout (15.9%)

See Limitations section for comprehensive failure analysis.

### Ethical Considerations

**Privacy:** Model observations may capture human presence or personal information. Deploy only in controlled environments with appropriate consent.

**Safety:** Requires human supervision. Not suitable for unsupervised deployment. 23 safety interventions recorded over 45 hours of operation (0.51/hour).

**Fairness:** Model trained primarily on Western household objects with English instructions. Generalization to diverse cultural contexts not evaluated.

**Environmental Impact:** Training requires ~4.5 GPU-hours per task (54 GPU-hours total). Estimated CO2 footprint: ~2.7 kg CO2e (assuming 50g CO2/kWh).

### Recommendations

**For Researchers:**
- Test thoroughly in your specific environment
- Report both successes and failures
- Consider domain adaptation if using different embodiment
- Share failure cases to improve community knowledge

**For Practitioners:**
- Start with low-risk tasks and soft objects
- Implement hardware safety measures (e-stop, padding)
- Maintain human supervision at all times
- Expect performance drop on out-of-distribution tasks

**For Educators:**
- Suitable for classroom demonstrations with supervision
- Good testbed for teaching VLA concepts
- Affordable platform (~$200 robot cost)
- Emphasize limitations and responsible use

### Model Versioning & Updates

**Current Version:** 1.0.0  
**Last Updated:** 2024-11  
**Changelog:**
- v1.0.0 (2024-11): Initial release

**Known Issues:**
- None currently reported

**Planned Updates:**
- Improved grasp detection (v1.1)
- Multi-modal sensing integration (v2.0)
- Uncertainty quantification (v2.0)

### Contact & Support

**Authors:** [TODO: Add your contact information]  
**Email:** first.author@university.edu  
**GitHub:** [TODO: Add repo link]  
**Issues:** Report issues on GitHub issue tracker

---

## Data Card

### Dataset Overview

{/* TODO: Update with your actual dataset details */}

**Dataset Name:** VLA-3DoF-Manipulation-v1  
**Version:** 1.0.0  
**Release Date:** 2024-11  
**License:** CC BY 4.0  
**DOI:** [TODO: Add DOI if available]

**Quick Description:**  
A dataset of 600 robotic manipulation episodes collected on a custom 3-DoF arm across 12 tasks. Includes RGB observations, proprioception, actions, and natural language instructions.

### Dataset Composition

**Size:**
- **Episodes:** 600 (50 per task × 12 tasks)
- **Timesteps:** ~180,000 (at 10Hz)
- **Duration:** 5 hours of robot interaction
- **Storage:** ~45 GB (uncompressed), ~12 GB (compressed)

**Modalities:**
- RGB images: 640×480, 30fps (downsampled to 224×224 for training)
- Proprioception: Joint angles (3), velocities (3), gripper state (1)
- Actions: Joint velocity commands (3), gripper command (1)
- Language: Task instructions (1 per episode, 5 paraphrases available)
- Metadata: Episode ID, task ID, success label, timestamp

**Data Splits:**
- **Training:** 480 episodes (40 per task)
- **Validation:** 60 episodes (5 per task)
- **Test:** 60 episodes (5 per task)
- **Note:** Test split uses different object instances

### Data Collection

**Collection Method:**
- **Teleoperation:** 360 episodes (60%) via gamepad controller
- **Online RL:** 240 episodes (40%) from policy rollouts
- **Collection Period:** November 2024 (2 weeks)
- **Collectors:** 2 researchers, both right-handed

**Collection Environment:**
- **Location:** Indoor robotics lab
- **Workspace:** 60×80cm tabletop
- **Lighting:** Overhead LED (400-600 lux)
- **Camera:** Intel RealSense D435i, fixed mount
- **Objects:** 30 household items (blocks, cups, markers, toys)

**Quality Control:**
- Manual inspection of all episodes
- Removed 43 episodes due to hardware errors
- Success labels verified by human annotator
- Consistent episode start/end states

### Data Content

**Tasks Included:**
1. Pick and place (50 episodes)
2. Stack blocks (50 episodes)
3. Push to goal (50 episodes)
4. Open drawer (50 episodes)
5. Close drawer (50 episodes)
6. Press button (50 episodes)
7. Pick by color (50 episodes)
8. Sort objects (50 episodes)
9. Reorient object (50 episodes)
10. Slide object (50 episodes)
11. Grasp from clutter (50 episodes)
12. Follow trajectory (50 episodes)

**Object Categories:**
- Wooden blocks (6 objects, various colors)
- Plastic cups (4 objects)
- Markers/pens (5 objects)
- Small toys (8 objects)
- Tools (screwdriver, wrench, 2 objects)
- Household items (5 objects)

**Instruction Diversity:**
- 12 base instructions (1 per task)
- 5 paraphrases per base instruction
- Total: 60 unique instruction strings
- Language: English only

### Data Distribution

**Episode Length Distribution:**
- Mean: 30.2s (302 timesteps)
- Std: 12.8s
- Min: 5.4s (press button task)
- Max: 61.7s (sort objects task)

**Success Rate:**
- Overall: 78.5% (471/600 episodes)
- Range: 64.2% (stack blocks) to 92.8% (press button)

**Object Distribution:**
- Balanced across tasks (each object appears 15-25 times)
- Color distribution: Red (28%), Blue (24%), Green (22%), Yellow (15%), Other (11%)

### Data Preprocessing

**Applied Preprocessing:**
- Image resizing: 640×480 → 224×224 (bilinear)
- Normalization: ImageNet mean/std for images
- Action clipping: Joint velocities clipped to [-1, 1]
- Temporal alignment: All modalities synced to 10Hz

**Provided Formats:**
- **Raw:** HDF5 files with full-resolution data
- **Processed:** TFRecord format for efficient training
- **Visualization:** MP4 videos for each episode

### Intended Use

**Primary Intended Uses:**
- Training VLA models for robotic manipulation
- Benchmarking embodiment adaptation methods
- Studying sample efficiency in robot learning
- Transfer learning research

**Out-of-Scope Uses:**
- Training models for different robot embodiments without adaptation
- Applications requiring high-DoF manipulation
- Safety-critical system development
- Commercial deployment without additional testing

### Limitations & Biases

**Dataset Limitations:**
- Small scale (600 episodes) compared to large VLA datasets
- Single environment (lab tabletop)
- Limited object diversity (30 objects)
- Single camera viewpoint
- English instructions only

**Potential Biases:**
- Collector bias: Both collectors right-handed, may affect grasp strategies
- Object bias: Primarily Western household items
- Lighting bias: Consistent overhead lighting, limited variation
- Success bias: 78.5% success rate may underrepresent failure modes

**Distribution Shift Concerns:**
- Different workspace layouts
- Novel object categories
- Varying lighting conditions
- Non-English instructions

### Data Quality

**Quality Assurance:**
- All episodes manually inspected
- Success labels verified by human
- Sensor calibration checked daily
- Anomaly detection removed 43 corrupted episodes

**Known Issues:**
- 12 episodes have minor image blur due to fast motion
- 8 episodes have partial object occlusion
- 3 episodes have brief gripper state sensor glitches (handled in preprocessing)

### Privacy & Ethics

**Privacy Considerations:**
- No human subjects in recorded data
- Lab environment, no personal information
- Object labels do not contain sensitive information

**Ethical Review:**
- No IRB required (no human subjects)
- Objects purchased commercially, no proprietary items
- Data collection followed lab safety protocols

**License & Attribution:**
- License: Creative Commons Attribution 4.0 (CC BY 4.0)
- Citation: See BibTeX in References section
- Acknowledgment requested for derived works

### Access & Maintenance

**Access:**
- **Download:** [TODO: Add Hugging Face or Zenodo link]
- **Format:** HDF5 (raw), TFRecord (processed), MP4 (videos)
- **Size:** 12 GB compressed download

**Maintenance Plan:**
- Bug fixes: As needed
- Version updates: If data issues discovered
- Community contributions: Welcome via pull requests
- Long-term hosting: Zenodo for permanent archival

**Versioning:**
- Current: v1.0.0
- Changelog: None (initial release)

### Contact

**Dataset Maintainers:** [TODO: Add your information]  
**Email:** first.author@university.edu  
**Issues:** Report data issues on GitHub

{/* TODO: Update all placeholder information with your actual details */}

